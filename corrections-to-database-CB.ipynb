{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Corrections to database - Cognitive Bias (CB)\"\n",
    "subtitle: \"Experimental data analysis (Oct / Nov 2023)\"\n",
    "date: \"now\"\n",
    "date-format: \"ddd MMM D, YYYY h:mm A\"\n",
    "author:\n",
    "  - name: \"Cathrynne Henshall\"\n",
    "    email: ponies@hillydale.com.au\n",
    "    affiliation: \n",
    "      - name: Charles Sturt University\n",
    "        url: www.csu.edu.au\n",
    "  - name: \"Michael J. Booth\"\n",
    "    email: michael@databooth.com.au\n",
    "    affiliation: \n",
    "      - name: DataBooth\n",
    "        url: www.databooth.com.au\n",
    "abstract: >\n",
    "  Parsing of the logfiles into DuckDB database for analysis.\n",
    "title-block-banner: true\n",
    "format:\n",
    "  html:\n",
    "    code-tools: true\n",
    "    code-fold: false\n",
    "    toc: true\n",
    "#  docx:\n",
    "#    toc: true # Include a table of contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of This Notebook\n",
    "\n",
    "This notebook serves to load data from the manually recorded corrections spreadsheet into tables in the CB database\n",
    "so that they can be applied to the data loaded from the log files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import duckdb\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from loguru import logger\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "from horse_logic.logfiles import (\n",
    "    EventCB,\n",
    "    ExperimentCB,\n",
    "    Logfile,\n",
    "    Logs,\n",
    "    ResponseCB,\n",
    "    TrialCB,\n",
    ")\n",
    "from horse_logic.utils import (\n",
    "    create_tables_from_sql_file,\n",
    "    export_data_to_csv,\n",
    "    set_custom_logger_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "import itables.options as opt\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(all_interactive=True, connected=False)  # Display Pandas dataframes in a more friendly paginated manner\n",
    "opt.pageLength = 20  # Display 20 rows per page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "set_custom_logger_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Subject information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "def get_subject_info():\n",
    "    HORSE_ORDER_XLSX = \"Cohort data for MB.xlsx\"\n",
    "    HORSE_ORDER_PATH = Path(\"../docs/from_CH\") \n",
    "    HORSE_ORDER_FILEPATH = HORSE_ORDER_PATH / HORSE_ORDER_XLSX\n",
    "\n",
    "    if HORSE_ORDER_FILEPATH.exists():\n",
    "        subject_df = pd.read_excel(HORSE_ORDER_FILEPATH)\n",
    "        logger.info(f\"Loaded horse order info from: {HORSE_ORDER_FILEPATH}\")\n",
    "\n",
    "        subject_df.rename({\"No\": \"subject_number\", \"Horse\": \"subject_name\"}, axis=1, inplace=True)  # Rename columns\n",
    "        subject_df[\"subject_name\"] = subject_df[\"subject_name\"].str.lower()     # Ensure lower case names for later subject lookup\n",
    "        return subject_df\n",
    "    else:\n",
    "        logger.error(f\"Horse order info not found: {HORSE_ORDER_FILEPATH}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = get_subject_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "EXPERIMENT_TYPE = \"CB\"    # or CB\n",
    "\n",
    "assert DATA_DIR.exists()\n",
    "\n",
    "DATA_DB  = DATA_DIR / f\"Experiments_{EXPERIMENT_TYPE}_2023-Q4.ddb\"  # DuckDB database name\n",
    "db_exists = DATA_DB.exists()\n",
    "\n",
    "logger.info(f\"Database file: {DATA_DB.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_log_file_name(horse_name, date, db_path):\n",
    "    try:\n",
    "        # Connect to DuckDB\n",
    "        con = duckdb.connect(db_path)\n",
    "\n",
    "        # Prepare and execute the query\n",
    "        query = \"\"\"\n",
    "        SELECT LogFileName\n",
    "        FROM ExperimentCBs\n",
    "        WHERE LOWER(SubjectName) = LOWER(?)\n",
    "          AND CAST(DateTime AS DATE) = ?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query with parameters\n",
    "        result = con.execute(query, [horse_name, date]).fetchall()\n",
    "        \n",
    "        con.close()\n",
    "\n",
    "        # Return the result (list of LogFileNames)\n",
    "        log_file_names = [row[0] for row in result]\n",
    "        logger.debug(f\"Query result for {horse_name} on {date}: {log_file_names}\")\n",
    "        return log_file_names\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in lookup_log_file_name: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "OUTPUT_DIR = DATA_DIR / f\"results/{EXPERIMENT_TYPE}\"\n",
    "\n",
    "assert OUTPUT_DIR.exists()\n",
    "logger.info(f\"Outputs dir: {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log_filename_column(df, db_path):\n",
    "    def lookup_wrapper(row):\n",
    "        horse_name = row['Horse'].lower()\n",
    "        date = row['Date'].date()  # Assuming 'Date' is already a datetime object\n",
    "        log_file_names = lookup_log_file_name(horse_name, date, db_path)\n",
    "        return log_file_names[0] if log_file_names else None\n",
    "\n",
    "    df['LogFilename'] = df.apply(lookup_wrapper, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECTIONS_WORKBOOK = Path(\"../docs/from_CH/Exp1 Errors.xlsx\")\n",
    "\n",
    "WORKSHEET_NAME = [\"CBHD_Times\", \"CBCSU_Times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, db_path):\n",
    "\n",
    "    columns_to_ffill = ['Date', 'Session', 'Horse']\n",
    "    df[columns_to_ffill] = df[columns_to_ffill].ffill()\n",
    "\n",
    "    df = df.drop(columns=['Unnamed: 9'], errors='ignore')\n",
    "    object_columns = df.select_dtypes(include=['object']).columns\n",
    "    df[object_columns] = df[object_columns].fillna('')\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Add LogFileName column\n",
    "    def get_log_filename(row):\n",
    "        horse_name = row['Horse'].lower()\n",
    "        date = row['Date'].date()\n",
    "        log_file_names = lookup_log_file_name(horse_name, date, db_path)\n",
    "        return ', '.join(log_file_names) if log_file_names else f\"No log file in CB database matches: {horse_name} / {date}\"\n",
    "\n",
    "    df['LogFileName'] = df.apply(get_log_filename, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_excel_sheet(input_file, sheet_name, output_file, db_path):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "    \n",
    "    # Preprocess the dataframe\n",
    "    df = preprocess_dataframe(df, db_path)\n",
    "    \n",
    "    # Create a copy of the original workbook\n",
    "    wb = openpyxl.load_workbook(input_file)\n",
    "    \n",
    "    # Create a new sheet for the processed data\n",
    "    processed_sheet_name = f\"{sheet_name}_processed\"\n",
    "    if processed_sheet_name in wb.sheetnames:\n",
    "        wb.remove(wb[processed_sheet_name])\n",
    "    ws_processed = wb.create_sheet(processed_sheet_name)\n",
    "    \n",
    "    # Write the processed dataframe to the new sheet\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws_processed.append(r)\n",
    "    \n",
    "    wb.save(output_file)\n",
    "    \n",
    "    print(f\"Processed data saved to '{output_file}' in sheet '{processed_sheet_name}'\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sheet = {}\n",
    "for sheet_name in WORKSHEET_NAME:\n",
    "    output_file = Path(f'{str(CORRECTIONS_WORKBOOK).replace(\".xlsx\", f\"_{sheet_name}.xlsx\")}')\n",
    "    processed_sheet[sheet_name] = process_excel_sheet(CORRECTIONS_WORKBOOK, sheet_name, output_file, str(DATA_DB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sheet[\"CBHD_Times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sheet[\"CBCSU_Times\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing this all together and putting the data in DuckDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# create_tables_from_sql_file(con, '../sql/create_cb_correction_tables_ddb.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
