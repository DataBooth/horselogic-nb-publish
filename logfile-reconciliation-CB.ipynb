{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Logfile Reconciliation - Cognitive Bias (CB)\"\n",
    "subtitle: \"Experimental data analysis (Oct / Nov 2023)\"\n",
    "date: \"now\"\n",
    "date-format: \"ddd MMM D, YYYY h:mm A\"\n",
    "author:\n",
    "  - name: \"Cathrynne Henshall\"\n",
    "    email: ponies@hillydale.com.au\n",
    "    affiliation: \n",
    "      - name: Charles Sturt University\n",
    "        url: www.csu.edu.au\n",
    "  - name: \"Michael J. Booth\"\n",
    "    email: michael@databooth.com.au\n",
    "    affiliation: \n",
    "      - name: DataBooth\n",
    "        url: www.databooth.com.au\n",
    "abstract: >\n",
    "  Reconciliation of the CB logfiles generated during the experiments.\n",
    "title-block-banner: true\n",
    "format:\n",
    "  html:\n",
    "    code-tools: true\n",
    "    code-fold: false\n",
    "    toc: true\n",
    "#  docx:\n",
    "#    toc: true # Include a table of contents\n",
    "execute:\n",
    "  enabled: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from loguru import logger\n",
    "\n",
    "from horse_logic.logfiles import Logs\n",
    "from horse_logic.utils import ProjectInfo, export_data_to_csv, set_custom_logger_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of This Notebook\n",
    "\n",
    "This notebook serves as an exploratory tool for examining the log files produced during the Cognitive Bias horse behavioural experiments conducted in October and November 2023. It facilitates the experimentation with text parsing techniques on the files before they are imported into a database. The primary objective is to reconcile which log files should be included in or excluded from the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment details and naming conventions\n",
    "\n",
    "There are two main types of Experiment:\n",
    "\n",
    "- Reward Prediction (RPE)\n",
    "- Cognitive Bias (CB)\n",
    "\n",
    "#### Logfile Exclusion rules\n",
    "\n",
    "Logfiles that are from test runs and also bad data need to be excluded from the analysis.\n",
    "\n",
    "Rules are **case-insensitive**. Files which satisfy the following conditions are excluded:\n",
    "\n",
    "- TODO\n",
    "  \n",
    "#### Problems with log file names during experiments\n",
    "\n",
    "- TODO\n",
    "\n",
    "#### Time differences\n",
    "\n",
    "For each trial we calculate the following time differences: \n",
    "\n",
    "\n",
    "### Cognitive Bias Experiments\n",
    "\n",
    "- Extract \"RIGHT\" or \"LEFT\" from the Comment field.\n",
    "- Also extract details from log file name\n",
    "- 1. Training experiments: \n",
    "  - Type 1\n",
    "  - Type 2\n",
    "-  2. Testing experiments: \n",
    "   -  Type 1\n",
    "   -  Type 2\n",
    "   -  Type 3\n",
    "   -  Type 4 (re-uses Type 1 with indicator to distinguish in Comment field)\n",
    "\n",
    "#### Time differences\n",
    "\n",
    "For each trial: \n",
    "\n",
    "Training Type 1 (randomised versus fixed):\n",
    "- `Start` datetime = Green button pressed and horse is released\n",
    "- Capture positive (`GO`) / negative (`NOGO`) response time subject to maximum cutoff time (e.g. 30 seconds)\n",
    "- In addition to left/right positioning of feed, there are also median, near positive and near negative positions.\n",
    "\n",
    "*TODO check with CH: Test only - be in all?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "import itables.options as opt\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(all_interactive=True, connected=False)  # Display Pandas dataframes in a more friendly paginated manner\n",
    "opt.pageLength = 20  # Display 20 rows per page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "PROJECT_DIR = ProjectInfo.get_root_dir()\n",
    "set_custom_logger_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# Custom notebook functions\n",
    "\n",
    "def display_sessions_by_subject(df_out, subject_df):\n",
    "    session_summary = []\n",
    "    for n_subject, subject_name in enumerate(subject_df[\"subject_name\"]):\n",
    "        df_ = df_out.drop(columns=[\"suffix\", \"subject_name\"])[df_out[\"subject_name\"] == subject_name]\n",
    "        n_session = len(df_)\n",
    "        session_summary.append((n_subject + 1, subject_name.capitalize(), n_session))\n",
    "        if n_session == 0:\n",
    "            logger.info(f\"Subject {subject_name}: No experiments conducted\")\n",
    "        else:\n",
    "            df_.loc[:, \"time_dff\"] = df_[\"datetime\"].diff()\n",
    "            display(Markdown(f\"### {n_subject+1}. {subject_name.capitalize()}: {n_session} session(s)\"))\n",
    "            display(df_)\n",
    "    return pd.DataFrame(session_summary, columns=[\"Subject number\", \"Subject name\", \"Session count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Subject order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "def get_subject_info():\n",
    "    HORSE_ORDER_XLSX = \"Cohort data for MB.xlsx\"\n",
    "    HORSE_ORDER_PATH = Path(\"../docs/from_CH\") \n",
    "    HORSE_ORDER_FILEPATH = HORSE_ORDER_PATH / HORSE_ORDER_XLSX\n",
    "\n",
    "    if HORSE_ORDER_FILEPATH.exists():\n",
    "        subject_df = pd.read_excel(HORSE_ORDER_FILEPATH)\n",
    "        logger.info(f\"Loaded horse order info from: {HORSE_ORDER_FILEPATH}\")\n",
    "\n",
    "        subject_df.rename({\"No\": \"subject_number\", \"Horse\": \"subject_name\"}, axis=1, inplace=True)  # Rename columns\n",
    "        subject_df[\"subject_name\"] = subject_df[\"subject_name\"].str.lower()     # Ensure lower case names for later subject lookup\n",
    "        return subject_df\n",
    "    else:\n",
    "        logger.error(f\"Horse order info not found: {HORSE_ORDER_FILEPATH}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "subject_df = get_subject_info()\n",
    "\n",
    "logger.info(f\"Subjects: {len(subject_df)}\")\n",
    "logger.info(subject_df[\"subject_name\"].to_list())\n",
    "\n",
    "# ensure lower case subject names for later lookup\n",
    "\n",
    "subject_df[\"subject_name\"] = subject_df[\"subject_name\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log file reconciliation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory information: log file data and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "LOGFILES_DIR = \"../data/results/zips/cb_data\"\n",
    "\n",
    "assert Path(LOGFILES_DIR).exists()\n",
    "logger.info(f\"Logfiles dir: {LOGFILES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# DATA_DIR = Path(\"../data\")\n",
    "# EXPERIMENT_TYPE = \"CB\"    # or RPE\n",
    "\n",
    "# assert DATA_DIR.exists()\n",
    "\n",
    "# DATA_DB  = DATA_DIR / f\"Experiments_{EXPERIMENT_TYPE}_2023-Q4.ddb\"  # DuckDB database name\n",
    "# db_exists = DATA_DB.exists()\n",
    "\n",
    "# logger.info(f\"Database file: {DATA_DB.resolve()}\")\n",
    "\n",
    "# | echo: false\n",
    "\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "EXPERIMENT_TYPE = \"CB\"  \n",
    "NOTEBOOK_DIR = Path.cwd()          # Current notebooks directory   \n",
    "\n",
    "assert DATA_DIR.exists()\n",
    "assert NOTEBOOK_DIR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "OUTPUT_DIR = DATA_DIR / f\"results/{EXPERIMENT_TYPE}\"\n",
    "\n",
    "assert OUTPUT_DIR.exists()\n",
    "logger.info(f\"Outputs dir: {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exclusions (rule-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules:\n",
    "\n",
    "- Ignore all CBF1 files - data will not be analysed (*CH: What does CBF1 mean? and similar*)\n",
    "- Ignore all Olive files (6 log files)\n",
    "- Ignore Maple CBT1 on 9 Oct (*CH: Did Maple have a different name?*)\n",
    "- Run check to see how many N bucket GO responses exceed 30s in first 3 days (*CH: Please explain*)\n",
    "\n",
    "\n",
    "- 29 `*test*.log` files (exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logs = Logs(path=LOGFILES_DIR, patterns=[\"*test*.log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[logfile.file_name for logfile in test_logs.logfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olive_logs = Logs(path=LOGFILES_DIR, patterns=[\"*olive*.log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olive_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[logfile.file_name for logfile in olive_logs.logfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maple_logs = Logs(path=LOGFILES_DIR, patterns=[\"*maple*.log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[logfile.file_name for logfile in maple_logs.logfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# Define the default set of log files to exclude before fine-tuning after\n",
    "\n",
    "logs = Logs(path=LOGFILES_DIR, patterns=[\"*_test*.log\", \"*olive*.log\"], include=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "logger.info(logs)\n",
    "logger.info(f\"Included logfiles: {len(logs.included_files)}\")\n",
    "logger.info(f\"Excluded logfiles: {len(logs.excluded_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_excluded = pd.DataFrame(logs.excluded_files, columns=[\"Excluded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_excluded.sort_values(by=\"Excluded\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Included log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_included = pd.DataFrame(logs.included_files, columns=[\"Included\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "export_data_to_csv(\n",
    "    df_included.sort_values(by=\"Included\").reset_index(drop=True),\n",
    "    \"List of files to be included in analysis\",\n",
    "    Path(OUTPUT_DIR) / \"included_files_CB.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_included.sort_values(by=\"Included\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_file_components = logs.create_filename_components_dataframe_cog_bias(logs.included_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_out = df_file_components.sort_values(by=[\"subject_name\", \"session_number\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions summary by subject name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "session_summary_df = display_sessions_by_subject(df_out, subject_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "export_data_to_csv(\n",
    "    session_summary_df,\n",
    "    \"Session overview (subject session counts)\",\n",
    "    Path(OUTPUT_DIR) / \"session_overview_CB.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_out.drop(labels=[\"suffix\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_out = df_out[[\"original_filename\", \"subject_name\", \"experiment_type\", \"session_number\", \"datetime\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# Calculate time differences between Experiments to look for anomalies\n",
    "\n",
    "df_out.loc[:, \"time_diff\"] = df_out[\"datetime\"].diff()\n",
    "\n",
    "# Convert session number to integers - not sure how we got some floats?\n",
    "\n",
    "df_out[\"session_number\"] = df_out[\"session_number\"].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# Export list of all excluded files - by rules and specific exclusions\n",
    "\n",
    "export_data_to_csv(\n",
    "    pd.DataFrame(logs.excluded_files, columns=[\"col1\"]).sort_values(by=\"col1\").reset_index(drop=True),\n",
    "    \"List of log files excluded\",\n",
    "    Path(OUTPUT_DIR) / \"all_excluded_files_CB.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "export_data_to_csv(\n",
    "    df_out, \"Experiment summary\", Path(OUTPUT_DIR) / \"experiment_summary_CB_2023_included.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# Export list of all included files - by rules and specific exclusions (inc Bonnie)\n",
    "\n",
    "export_data_to_csv(\n",
    "    pd.DataFrame(logs.included_files, columns=[\"col1\"]).sort_values(by=\"col1\").reset_index(drop=True),\n",
    "    \"File list of log files included\",\n",
    "    Path(OUTPUT_DIR) / \"all_included_files_CB.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
