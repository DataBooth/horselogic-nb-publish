[
  {
    "objectID": "logfile-reconciliation-CB.html",
    "href": "logfile-reconciliation-CB.html",
    "title": "Logfile Reconciliation - Cognitive Bias (CB)",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log files produced during the Cognitive Bias horse behavioural experiments conducted in October and November 2023. It facilitates the experimentation with text parsing techniques on the files before they are imported into a database. The primary objective is to reconcile which log files should be included in or excluded from the analysis."
  },
  {
    "objectID": "logfile-reconciliation-CB.html#purpose-of-this-notebook",
    "href": "logfile-reconciliation-CB.html#purpose-of-this-notebook",
    "title": "Logfile Reconciliation - Cognitive Bias (CB)",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log files produced during the Cognitive Bias horse behavioural experiments conducted in October and November 2023. It facilitates the experimentation with text parsing techniques on the files before they are imported into a database. The primary objective is to reconcile which log files should be included in or excluded from the analysis."
  },
  {
    "objectID": "logfile-reconciliation-CB.html#experiment-details-and-naming-conventions",
    "href": "logfile-reconciliation-CB.html#experiment-details-and-naming-conventions",
    "title": "Logfile Reconciliation - Cognitive Bias (CB)",
    "section": "Experiment details and naming conventions",
    "text": "Experiment details and naming conventions\nThere are two main types of Experiment:\n\nReward Prediction (RPE)\nCognitive Bias (CB)\n\n\nLogfile Exclusion rules\nLogfiles that are from test runs and also bad data need to be excluded from the analysis.\nRules are case-insensitive. Files which satisfy the following conditions are excluded:\n\nTODO\n\n\n\nProblems with log file names during experiments\n\nTODO\n\n\n\nTime differences\nFor each trial we calculate the following time differences:\n\n\nCognitive Bias Experiments\n\nExtract “RIGHT” or “LEFT” from the Comment field.\nAlso extract details from log file name\n\nTraining experiments:\n\n\nType 1\nType 2\n\n\nTesting experiments:\n\n\nType 1\nType 2\nType 3\nType 4 (re-uses Type 1 with indicator to distinguish in Comment field)\n\n\n\nTime differences\nFor each trial:\nTraining Type 1 (randomised versus fixed): - Start datetime = Green button pressed and horse is released - Capture positive (GO) / negative (NOGO) response time subject to maximum cutoff time (e.g. 30 seconds) - In addition to left/right positioning of feed, there are also median, near positive and near negative positions.\nTODO check with CH: Test only - be in all?\n\n\n\nLoad Subject order"
  },
  {
    "objectID": "logfile-reconciliation-CB.html#log-file-reconciliation",
    "href": "logfile-reconciliation-CB.html#log-file-reconciliation",
    "title": "Logfile Reconciliation - Cognitive Bias (CB)",
    "section": "Log file reconciliation",
    "text": "Log file reconciliation\n\nDirectory information: log file data and outputs\n\n\nInitial exclusions (rule-based)\nRules:\n\nIgnore all CBF1 files - data will not be analysed (CH: What does CBF1 mean? and similar)\nIgnore all Olive files (6 log files)\nIgnore Maple CBT1 on 9 Oct (CH: Did Maple have a different name?)\nRun check to see how many N bucket GO responses exceed 30s in first 3 days (CH: Please explain)\n29 *test*.log files (exclude)\n\n\ntest_logs = Logs(path=LOGFILES_DIR, patterns=[\"*test*.log\"])\n\n\n[logfile.file_name for logfile in test_logs.logfiles]\n\n\ntest_logs\n\n\nolive_logs = Logs(path=LOGFILES_DIR, patterns=[\"*olive*.log\"])\n\n\nolive_logs\n\n\n[logfile.file_name for logfile in olive_logs.logfiles]\n\n\nmaple_logs = Logs(path=LOGFILES_DIR, patterns=[\"*maple*.log\"])\n\n\n[logfile.file_name for logfile in maple_logs.logfiles]\n\n\nIncluded log files\n\n\n\nSessions summary by subject name\n\nsession_summary_df"
  },
  {
    "objectID": "database-queries-CB.html",
    "href": "database-queries-CB.html",
    "title": "CB Database Example Queries",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log file data from the horse behavioural experiments conducted in October and November 2023 that are loaded into a local DuckDB database using logfile-to-database-CB.ipynb.\nIt facilitates loading and querying the data from the database using some example SQL queries.\n\n\n\n\n\n\n\n\n\n\n\nShow the tables in the database - should be EventCBs, ExperimentCBs, ResponseCBs and TrialCBs.\n\n# Cross-check queries\n\ncon.sql(\"SHOW TABLES;\")\n\n\n\n\n\ncon.sql(\"SELECT DISTINCT EventType FROM EventCBs ORDER BY EventType\")\n\n\n\n\n\nexperiments_df = con.sql(\"SELECT * FROM ExperimentCBs\").df()\n\n\nexperiments_df;\n\n\n\n\n\nfilenames_df = con.sql(\"SELECT LogFilename from ExperimentCBs ORDER By LogFilename\").df()\n\n\nfilenames_df.iloc[0, 0]\n\n\n\n\n\ndef display_file_contents(file_path):\n    try:\n        with open(file_path, \"r\") as file:\n            contents = file.read()\n            print(contents)\n    except FileNotFoundError:\n        print(f\"Error: The file '{file_path}' was not found.\")\n    except IOError:\n        print(f\"Error: There was an issue reading the file '{file_path}'.\")\n\n\nexample_log = DATA_DIR / \"results\" / \"zips\" / \"cb_data\" / filenames_df.iloc[0, 0]\n\n\ndef sql_recreate_logfile(filename):\n    return f\"\"\"\n        WITH log_events AS (\n        SELECT \n            e.EventTime,\n            e.EventType,\n            t.TrialNumber,\n            t.ResponseType,\n            t.TrialSubType,\n            r.ResponseTime\n        FROM EventCBs e\n        LEFT JOIN TrialCBs t ON e.TrialID = t.TrialID\n        LEFT JOIN ResponseCBs r ON e.TrialID = r.TrialID\n        JOIN ExperimentCBs ex ON t.ExperimentID = ex.ExperimentID\n        WHERE ex.LogFileName = '{filename}'\n        ORDER BY e.EventTime\n        )\n        SELECT \n        strftime('%Y-%m-%d %H:%M:%S.%f', EventTime) || ': ' ||\n        CASE \n            WHEN EventType LIKE 'GO for Trial%' OR EventType LIKE 'NOGO for Trial%'\n            THEN EventType || \n                CASE \n                    WHEN ResponseTime IS NOT NULL THEN ' - response time ' || CAST(ResponseTime AS TEXT)\n                    ELSE ''\n                END\n            ELSE EventType\n        END AS {sanitize_filename_for_sql(filename)}\n        FROM log_events;\n    \"\"\"\n\n\nexample_log.name\n\n\nimport re\n\ndef sanitize_filename_for_sql(filename):\n    # Remove the file extension\n    name_without_ext = filename.rsplit('.', 1)[0]\n    \n    # Replace spaces and hyphens with underscores\n    name_underscored = name_without_ext.replace(' ', '_').replace('-', '_')\n    \n    # Remove any characters that are not alphanumeric or underscore\n    sanitized = re.sub(r'[^\\w]', '', name_underscored)\n    \n    # Ensure the name starts with a letter or underscore\n    if not sanitized[0].isalpha() and sanitized[0] != '_':\n        sanitized = '_' + sanitized\n    \n    # Truncate to a reasonable length if needed (e.g., 63 characters)\n    sanitized = sanitized[:63]\n    \n    return sanitized\n\n\nsql_recreate_logfile(example_log.name)\n\n\ncon.sql(sql_recreate_logfile(example_log.name))\n\n\ndata_dir = example_log.parent\n\n\nexample_log\n\n\n# Compare the actual file contents\n\ndisplay_file_contents(example_log)\n\n\n\n\n\n\n\ndeep_dive_filename = \"Experiment_2023-10-09T17:45:46.540359_apollo_5_Training randomised Type 1.log\"\n\n\ndef get_experiment_details(filename):\n    query = f\"\"\"\n        SELECT\n            ExperimentID, \n            Cohort,\n            SubjectName, \n            SubjectNumber, \n            SessionNumber,\n            ExperimentType, \n            Comment, \n            DateTime, \n            LogFileName\n        FROM ExperimentCBs \n        WHERE LogFileName = '{filename}';\n    \"\"\"\n    return con.sql(query).df()\n\n\ndef get_trials_details(filename):\n    query = f\"\"\"\n        SELECT t.*\n        FROM TrialCBs t\n        JOIN ExperimentCBs e ON t.ExperimentID = e.ExperimentID\n        WHERE e.LogFileName = '{filename}';\n    \"\"\"\n    return con.sql(query).df()\n\n\ndef get_events_details(filename):\n    query = f\"\"\"\n        SELECT ev.*\n        FROM EventCBs ev\n        JOIN TrialCBs t ON ev.TrialID = t.TrialID\n        JOIN ExperimentCBs e ON t.ExperimentID = e.ExperimentID\n        WHERE e.LogFileName = '{filename}';\n    \"\"\"\n    return con.sql(query).df()\n\n\ndef get_responses_details(filename):\n    query = f\"\"\"\n        SELECT r.*,\n        t.TrialNumber,\n        t.ResponseType,\n        t.TrialSubType,\n        t.Direction,\n        t.CriterionType,\n        t.CriterionCount,\n        t.TrialStartTime,\n        t.TrialEndTime,\n        t.SessionType\n        FROM ResponseCBs r\n        JOIN TrialCBs t ON r.TrialID = t.TrialID\n        JOIN ExperimentCBs e ON t.ExperimentID = e.ExperimentID\n        WHERE e.LogFileName = '{filename}';\n    \"\"\"\n    return con.sql(query).df()\n\n\ndef compare_experiment_data(filename):\n    display(Markdown(f\"### Compare: {filename}\"))\n    display(get_experiment_details(filename))\n    display(get_trials_details(filename))\n    display(get_events_details(filename))  \n    display(get_responses_details(filename))\n    display(Markdown(\"### Original logfile\\n\"))\n    display_file_contents(data_dir / filename)\n    \n\n\ncompare_experiment_data(deep_dive_filename)\n\n\ncompare_experiment_data(\"Experiment_2023-10-11T10:26:14.965471_atom_8_Test Type 1.log\")\n\n\n\n\n\nTo avoid file lock errors.\n\n# con.close()\n\n\n\n\nSELECT AVG(r.ResponseTime) as AvgResponseTime\nFROM Trial t\nJOIN Response r ON t.TrialID = r.TrialID\nWHERE t.ExperimentID = 1 AND t.ResponseType = 'GO';\nSELECT TrialSubType, COUNT(*) as SuccessfulTrials\nFROM Trial\nWHERE CriterionType = 'positive'\nGROUP BY TrialSubType;\nExperiment duration\nSELECT \n    e.ExperimentID,\n    MIN(ev.EventTime) as StartTime,\n    MAX(ev.EventTime) as EndTime,\n    TIMESTAMPDIFF(SECOND, MIN(ev.EventTime), MAX(ev.EventTime)) as DurationSeconds\nFROM Experiment e\nJOIN Trial t ON e.ExperimentID = t.ExperimentID\nJOIN Event ev ON t.TrialID = ev.TrialID\nGROUP BY e.ExperimentID;"
  },
  {
    "objectID": "database-queries-CB.html#purpose-of-this-notebook",
    "href": "database-queries-CB.html#purpose-of-this-notebook",
    "title": "CB Database Example Queries",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log file data from the horse behavioural experiments conducted in October and November 2023 that are loaded into a local DuckDB database using logfile-to-database-CB.ipynb.\nIt facilitates loading and querying the data from the database using some example SQL queries.\n\n\n\n\n\n\n\n\n\n\n\nShow the tables in the database - should be EventCBs, ExperimentCBs, ResponseCBs and TrialCBs.\n\n# Cross-check queries\n\ncon.sql(\"SHOW TABLES;\")\n\n\n\n\n\ncon.sql(\"SELECT DISTINCT EventType FROM EventCBs ORDER BY EventType\")\n\n\n\n\n\nexperiments_df = con.sql(\"SELECT * FROM ExperimentCBs\").df()\n\n\nexperiments_df;\n\n\n\n\n\nfilenames_df = con.sql(\"SELECT LogFilename from ExperimentCBs ORDER By LogFilename\").df()\n\n\nfilenames_df.iloc[0, 0]\n\n\n\n\n\ndef display_file_contents(file_path):\n    try:\n        with open(file_path, \"r\") as file:\n            contents = file.read()\n            print(contents)\n    except FileNotFoundError:\n        print(f\"Error: The file '{file_path}' was not found.\")\n    except IOError:\n        print(f\"Error: There was an issue reading the file '{file_path}'.\")\n\n\nexample_log = DATA_DIR / \"results\" / \"zips\" / \"cb_data\" / filenames_df.iloc[0, 0]\n\n\ndef sql_recreate_logfile(filename):\n    return f\"\"\"\n        WITH log_events AS (\n        SELECT \n            e.EventTime,\n            e.EventType,\n            t.TrialNumber,\n            t.ResponseType,\n            t.TrialSubType,\n            r.ResponseTime\n        FROM EventCBs e\n        LEFT JOIN TrialCBs t ON e.TrialID = t.TrialID\n        LEFT JOIN ResponseCBs r ON e.TrialID = r.TrialID\n        JOIN ExperimentCBs ex ON t.ExperimentID = ex.ExperimentID\n        WHERE ex.LogFileName = '{filename}'\n        ORDER BY e.EventTime\n        )\n        SELECT \n        strftime('%Y-%m-%d %H:%M:%S.%f', EventTime) || ': ' ||\n        CASE \n            WHEN EventType LIKE 'GO for Trial%' OR EventType LIKE 'NOGO for Trial%'\n            THEN EventType || \n                CASE \n                    WHEN ResponseTime IS NOT NULL THEN ' - response time ' || CAST(ResponseTime AS TEXT)\n                    ELSE ''\n                END\n            ELSE EventType\n        END AS {sanitize_filename_for_sql(filename)}\n        FROM log_events;\n    \"\"\"\n\n\nexample_log.name\n\n\nimport re\n\ndef sanitize_filename_for_sql(filename):\n    # Remove the file extension\n    name_without_ext = filename.rsplit('.', 1)[0]\n    \n    # Replace spaces and hyphens with underscores\n    name_underscored = name_without_ext.replace(' ', '_').replace('-', '_')\n    \n    # Remove any characters that are not alphanumeric or underscore\n    sanitized = re.sub(r'[^\\w]', '', name_underscored)\n    \n    # Ensure the name starts with a letter or underscore\n    if not sanitized[0].isalpha() and sanitized[0] != '_':\n        sanitized = '_' + sanitized\n    \n    # Truncate to a reasonable length if needed (e.g., 63 characters)\n    sanitized = sanitized[:63]\n    \n    return sanitized\n\n\nsql_recreate_logfile(example_log.name)\n\n\ncon.sql(sql_recreate_logfile(example_log.name))\n\n\ndata_dir = example_log.parent\n\n\nexample_log\n\n\n# Compare the actual file contents\n\ndisplay_file_contents(example_log)\n\n\n\n\n\n\n\ndeep_dive_filename = \"Experiment_2023-10-09T17:45:46.540359_apollo_5_Training randomised Type 1.log\"\n\n\ndef get_experiment_details(filename):\n    query = f\"\"\"\n        SELECT\n            ExperimentID, \n            Cohort,\n            SubjectName, \n            SubjectNumber, \n            SessionNumber,\n            ExperimentType, \n            Comment, \n            DateTime, \n            LogFileName\n        FROM ExperimentCBs \n        WHERE LogFileName = '{filename}';\n    \"\"\"\n    return con.sql(query).df()\n\n\ndef get_trials_details(filename):\n    query = f\"\"\"\n        SELECT t.*\n        FROM TrialCBs t\n        JOIN ExperimentCBs e ON t.ExperimentID = e.ExperimentID\n        WHERE e.LogFileName = '{filename}';\n    \"\"\"\n    return con.sql(query).df()\n\n\ndef get_events_details(filename):\n    query = f\"\"\"\n        SELECT ev.*\n        FROM EventCBs ev\n        JOIN TrialCBs t ON ev.TrialID = t.TrialID\n        JOIN ExperimentCBs e ON t.ExperimentID = e.ExperimentID\n        WHERE e.LogFileName = '{filename}';\n    \"\"\"\n    return con.sql(query).df()\n\n\ndef get_responses_details(filename):\n    query = f\"\"\"\n        SELECT r.*,\n        t.TrialNumber,\n        t.ResponseType,\n        t.TrialSubType,\n        t.Direction,\n        t.CriterionType,\n        t.CriterionCount,\n        t.TrialStartTime,\n        t.TrialEndTime,\n        t.SessionType\n        FROM ResponseCBs r\n        JOIN TrialCBs t ON r.TrialID = t.TrialID\n        JOIN ExperimentCBs e ON t.ExperimentID = e.ExperimentID\n        WHERE e.LogFileName = '{filename}';\n    \"\"\"\n    return con.sql(query).df()\n\n\ndef compare_experiment_data(filename):\n    display(Markdown(f\"### Compare: {filename}\"))\n    display(get_experiment_details(filename))\n    display(get_trials_details(filename))\n    display(get_events_details(filename))  \n    display(get_responses_details(filename))\n    display(Markdown(\"### Original logfile\\n\"))\n    display_file_contents(data_dir / filename)\n    \n\n\ncompare_experiment_data(deep_dive_filename)\n\n\ncompare_experiment_data(\"Experiment_2023-10-11T10:26:14.965471_atom_8_Test Type 1.log\")\n\n\n\n\n\nTo avoid file lock errors.\n\n# con.close()\n\n\n\n\nSELECT AVG(r.ResponseTime) as AvgResponseTime\nFROM Trial t\nJOIN Response r ON t.TrialID = r.TrialID\nWHERE t.ExperimentID = 1 AND t.ResponseType = 'GO';\nSELECT TrialSubType, COUNT(*) as SuccessfulTrials\nFROM Trial\nWHERE CriterionType = 'positive'\nGROUP BY TrialSubType;\nExperiment duration\nSELECT \n    e.ExperimentID,\n    MIN(ev.EventTime) as StartTime,\n    MAX(ev.EventTime) as EndTime,\n    TIMESTAMPDIFF(SECOND, MIN(ev.EventTime), MAX(ev.EventTime)) as DurationSeconds\nFROM Experiment e\nJOIN Trial t ON e.ExperimentID = t.ExperimentID\nJOIN Event ev ON t.TrialID = ev.TrialID\nGROUP BY e.ExperimentID;"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "TODO\n\n\nThe primary goal of this repository is to …\n\n\n\nTODO."
  },
  {
    "objectID": "about.html#purpose",
    "href": "about.html#purpose",
    "title": "About",
    "section": "",
    "text": "The primary goal of this repository is to …"
  },
  {
    "objectID": "about.html#features",
    "href": "about.html#features",
    "title": "About",
    "section": "",
    "text": "TODO."
  },
  {
    "objectID": "corrections-to-database-CB.html",
    "href": "corrections-to-database-CB.html",
    "title": "Corrections to database - Cognitive Bias (CB)",
    "section": "",
    "text": "This notebook serves to load data from the manually recorded corrections spreadsheet into tables in the CB database so that they can be applied to the data loaded from the log files.\n\n\n\nsubject_df = get_subject_info()\n\n\n\n\n\ndef lookup_log_file_name(horse_name, date, db_path):\n    try:\n        # Connect to DuckDB\n        con = duckdb.connect(db_path)\n\n        # Prepare and execute the query\n        query = \"\"\"\n        SELECT LogFileName\n        FROM ExperimentCBs\n        WHERE LOWER(SubjectName) = LOWER(?)\n          AND CAST(DateTime AS DATE) = ?\n        \"\"\"\n        \n        # Execute the query with parameters\n        result = con.execute(query, [horse_name, date]).fetchall()\n        \n        con.close()\n\n        # Return the result (list of LogFileNames)\n        log_file_names = [row[0] for row in result]\n        logger.debug(f\"Query result for {horse_name} on {date}: {log_file_names}\")\n        return log_file_names\n    except Exception as e:\n        logger.error(f\"Error in lookup_log_file_name: {str(e)}\")\n        return []"
  },
  {
    "objectID": "corrections-to-database-CB.html#purpose-of-this-notebook",
    "href": "corrections-to-database-CB.html#purpose-of-this-notebook",
    "title": "Corrections to database - Cognitive Bias (CB)",
    "section": "",
    "text": "This notebook serves to load data from the manually recorded corrections spreadsheet into tables in the CB database so that they can be applied to the data loaded from the log files.\n\n\n\nsubject_df = get_subject_info()\n\n\n\n\n\ndef lookup_log_file_name(horse_name, date, db_path):\n    try:\n        # Connect to DuckDB\n        con = duckdb.connect(db_path)\n\n        # Prepare and execute the query\n        query = \"\"\"\n        SELECT LogFileName\n        FROM ExperimentCBs\n        WHERE LOWER(SubjectName) = LOWER(?)\n          AND CAST(DateTime AS DATE) = ?\n        \"\"\"\n        \n        # Execute the query with parameters\n        result = con.execute(query, [horse_name, date]).fetchall()\n        \n        con.close()\n\n        # Return the result (list of LogFileNames)\n        log_file_names = [row[0] for row in result]\n        logger.debug(f\"Query result for {horse_name} on {date}: {log_file_names}\")\n        return log_file_names\n    except Exception as e:\n        logger.error(f\"Error in lookup_log_file_name: {str(e)}\")\n        return []"
  },
  {
    "objectID": "corrections-to-database-CB.html#corrections-workbook",
    "href": "corrections-to-database-CB.html#corrections-workbook",
    "title": "Corrections to database - Cognitive Bias (CB)",
    "section": "Corrections workbook",
    "text": "Corrections workbook\n\ndef add_log_filename_column(df, db_path):\n    def lookup_wrapper(row):\n        horse_name = row['Horse'].lower()\n        date = row['Date'].date()  # Assuming 'Date' is already a datetime object\n        log_file_names = lookup_log_file_name(horse_name, date, db_path)\n        return log_file_names[0] if log_file_names else None\n\n    df['LogFilename'] = df.apply(lookup_wrapper, axis=1)\n    return df\n\n\nCORRECTIONS_WORKBOOK = Path(\"../docs/from_CH/Exp1 Errors.xlsx\")\n\nWORKSHEET_NAME = [\"CBHD_Times\", \"CBCSU_Times\"]\n\n\ndef preprocess_dataframe(df, db_path):\n\n    columns_to_ffill = ['Date', 'Session', 'Horse']\n    df[columns_to_ffill] = df[columns_to_ffill].ffill()\n\n    df = df.drop(columns=['Unnamed: 9'], errors='ignore')\n    object_columns = df.select_dtypes(include=['object']).columns\n    df[object_columns] = df[object_columns].fillna('')\n\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Add LogFileName column\n    def get_log_filename(row):\n        horse_name = row['Horse'].lower()\n        date = row['Date'].date()\n        log_file_names = lookup_log_file_name(horse_name, date, db_path)\n        return ', '.join(log_file_names) if log_file_names else f\"No log file in CB database matches: {horse_name} / {date}\"\n\n    df['LogFileName'] = df.apply(get_log_filename, axis=1)\n\n    return df\n\n\ndef process_excel_sheet(input_file, sheet_name, output_file, db_path):\n    # Read the Excel file\n    df = pd.read_excel(input_file, sheet_name=sheet_name)\n    \n    # Preprocess the dataframe\n    df = preprocess_dataframe(df, db_path)\n    \n    # Create a copy of the original workbook\n    wb = openpyxl.load_workbook(input_file)\n    \n    # Create a new sheet for the processed data\n    processed_sheet_name = f\"{sheet_name}_processed\"\n    if processed_sheet_name in wb.sheetnames:\n        wb.remove(wb[processed_sheet_name])\n    ws_processed = wb.create_sheet(processed_sheet_name)\n    \n    # Write the processed dataframe to the new sheet\n    for r in dataframe_to_rows(df, index=False, header=True):\n        ws_processed.append(r)\n    \n    wb.save(output_file)\n    \n    print(f\"Processed data saved to '{output_file}' in sheet '{processed_sheet_name}'\")\n    \n    return df\n\n\nprocessed_sheet = {}\nfor sheet_name in WORKSHEET_NAME:\n    output_file = Path(f'{str(CORRECTIONS_WORKBOOK).replace(\".xlsx\", f\"_{sheet_name}.xlsx\")}')\n    processed_sheet[sheet_name] = process_excel_sheet(CORRECTIONS_WORKBOOK, sheet_name, output_file, str(DATA_DB))\n\n\nprocessed_sheet[\"CBHD_Times\"]\n\n\nprocessed_sheet[\"CBCSU_Times\"]\n\n\nBringing this all together and putting the data in DuckDB database"
  },
  {
    "objectID": "logfile-reconciliation-RPE.html",
    "href": "logfile-reconciliation-RPE.html",
    "title": "Logfile Reconciliation - Reward Prediction Error (RPE)",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log files produced during the horse behavioural experiments conducted in October and November 2023. It facilitates the experimentation with text parsing techniques on the files before they are imported into a DuckDB database.\nThe primary objectives are:\n\nTo reconcile which log files should be included in or excluded from the analysis.\nTo conduct experiments with regular expressions (regex) aimed at extracting pertinent data and fields from the log files."
  },
  {
    "objectID": "logfile-reconciliation-RPE.html#purpose",
    "href": "logfile-reconciliation-RPE.html#purpose",
    "title": "Logfile Reconciliation - Reward Prediction Error (RPE)",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log files produced during the horse behavioural experiments conducted in October and November 2023. It facilitates the experimentation with text parsing techniques on the files before they are imported into a DuckDB database.\nThe primary objectives are:\n\nTo reconcile which log files should be included in or excluded from the analysis.\nTo conduct experiments with regular expressions (regex) aimed at extracting pertinent data and fields from the log files."
  },
  {
    "objectID": "logfile-reconciliation-RPE.html#experiment-details-and-naming-conventions",
    "href": "logfile-reconciliation-RPE.html#experiment-details-and-naming-conventions",
    "title": "Logfile Reconciliation - Reward Prediction Error (RPE)",
    "section": "Experiment details and naming conventions",
    "text": "Experiment details and naming conventions\nThere are two main types of Experiment:\n\nReward Prediction (RPE)\nCognitive Bias (CB)\n\nThe RPE have the following subtypes:\n\nRPE-A : acquisition of response\nRPE-H : habit formation\nRPE-E : extinction of response\nRPE-ER : extinction prior to reinstatement of response\nRPE-R : reinstatement of response\n\n\nRPE-type experiments\nA new experiment type (RPE-ER) was created during the experiments which was not in the original specification.\n\nLogfile Exclusion rules\nLogfiles that are from test runs and also bad data need to be excluded from the analysis.\nRules are case-insensitive. Files which satisfy the following conditions are excluded:\n\nAll files with _TEST_ as the subject name\nAll files with _FRECKLE_ as the subject name\nSome files with _BONNIE_ (14 legitimate files - TODO: CH to confirm details)\nAll files with test in the Comment field\nPossibly some logs with very short run-times - TODO: CH to confirm the files that have been identified\nAll files with _OLIVE_ as the subject name - probably discard, treat as optional for now (TODO: CH to confirm treatment)\n\n\n\nProblems with log file names during experiments\n\nAppears that for greater than 20 trials (subjects?) the value in the filename was reported as NaN. e.g. see pumba experiments.\nOther NaNs? There are 31 files with NaN (some are restarts) - TODO: Confirm with CH why were there restarts? Why not “new” experiment?\n\n\n\nTime differences\nFor each trial we calculate the following time differences:\n\nTime delta: (touch datetime - start tone datetime)\nTime delta: (Next start datetime - dispense of pellets datetime)\nTime delta: (Dispense final pellets datetime - start tone datetime)\n\nWe use item 3 as cross check on the consistency of previous time deltas.\nThese calculated quantities are the same for all RPE-type experiments.\n\n\n\nLoad Subject info"
  },
  {
    "objectID": "logfile-reconciliation-RPE.html#directory-information-log-file-data-and-outputs",
    "href": "logfile-reconciliation-RPE.html#directory-information-log-file-data-and-outputs",
    "title": "Logfile Reconciliation - Reward Prediction Error (RPE)",
    "section": "Directory information: log file data and outputs",
    "text": "Directory information: log file data and outputs"
  },
  {
    "objectID": "logfile-reconciliation-RPE.html#log-file-reconciliation",
    "href": "logfile-reconciliation-RPE.html#log-file-reconciliation",
    "title": "Logfile Reconciliation - Reward Prediction Error (RPE)",
    "section": "Log file reconciliation",
    "text": "Log file reconciliation\n\nInitial exclusions (rule-based)\n\n\nIncluded log files\nInitial list of log files that are included based on rules specified above.\n\n\nSessions summary by subject name\n\n\nSession Summary - by Subject name and Session count\n\n\nSpecific analysis for Bonnie log files\nUsed some Bonnie log files for testing - not all experiments are valid to include\n\nextra_bonnie_logs_include = [\n    \"Experiment_2023-10-11T16:56:09.875604_bonnie_58_RPE-A.log\",\n    \"Experiment_2023-10-16T16:55:54.839279_bonnie_68_RPE-A.log\",\n    \"Experiment_2023-10-19T14:37:56.973616_bonnie_72_RPE-H.log\",\n]\n\n# Include these previously excluded files as per Signal chat 30 June 2024\n\n\n\nWrite out final lists of excluded and included log files\nKey output file here is all_included_files.csv which defines all of the log files that will be included in the load of data to the DuckDB database for further analysis.\nThe next notebook to run is notebooks/logfile-to-database-RPE.ipynb."
  },
  {
    "objectID": "logfile-to-database-CB.html",
    "href": "logfile-to-database-CB.html",
    "title": "Logfiles to CB database",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log files produced during the horse behavioural experiments conducted in October and November 2023. It facilitates the experimentation with text parsing techniques on the files before they are imported into a database. The primary objectives are:\n\nTo conduct experiments with regular expressions (regex) aimed at extracting pertinent data and fields from the log files.\n\n\n\n\nsubject_df = get_subject_info()"
  },
  {
    "objectID": "logfile-to-database-CB.html#purpose-of-this-notebook",
    "href": "logfile-to-database-CB.html#purpose-of-this-notebook",
    "title": "Logfiles to CB database",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log files produced during the horse behavioural experiments conducted in October and November 2023. It facilitates the experimentation with text parsing techniques on the files before they are imported into a database. The primary objectives are:\n\nTo conduct experiments with regular expressions (regex) aimed at extracting pertinent data and fields from the log files.\n\n\n\n\nsubject_df = get_subject_info()"
  },
  {
    "objectID": "logfile-to-database-CB.html#log-file-reconciliation",
    "href": "logfile-to-database-CB.html#log-file-reconciliation",
    "title": "Logfiles to CB database",
    "section": "Log file reconciliation",
    "text": "Log file reconciliation\n\nDirectory information: log file data and outputs\n\n\nLoad logfiles from CSV list of included files\nAs determined in logfiles-reconciliation-cb.ipynb\n\n\nExplore regex parsing of a single example log file\nAnalysis code is in logfiles.py. Explore parsing log information for specific example defined below.\n\n# JUst choose one of the file names for testing\n\nlog_eg = [log.file_name for log in logs.logfiles][0]\n\n\n# Override with specific log file\n\nlog_eg = \"Experiment_2023-11-15T00:39:50.244185_clover_nan_Test Type 1.log\"\n\n\nlogfile_eg\n\n\nlog_eg\n\n\n\nData structures\nThese are created as dataclasses in Python (in logfiles.py) and TABLES in SQL (sql/create_experiment_tables_ddb.sql).\n\nExperiment table\n\n\ne.g. Parse experiment details and comment from log file\n\n\nTrials and Events tables\n\nlog_eg.parse_filename_components_cog_bias().LogFileName\n\n\n\ne.g. Parsing of trials and events info from log file\n\nlog_eg.parse_trials_and_events_cog_bias()\n\n\nlog_eg.parse_comments()\n\n\nfrom dataclasses import asdict\n\ndef convert_parsed_data_to_dataframes(parsed_data):\n    if parsed_data is None:\n        return None\n\n    # Convert experiment to DataFrame\n    experiment_df = pd.DataFrame([asdict(parsed_data['experiment'])])\n\n    # Convert trials to DataFrame\n    trials_df = pd.DataFrame([asdict(trial) for trial in parsed_data['trials']])\n\n    # Convert responses to DataFrame\n    responses_df = pd.DataFrame([asdict(response) for response in parsed_data['responses']])\n\n    # Convert events to DataFrame\n    events_df = pd.DataFrame([asdict(event) for event in parsed_data['events']])\n\n    return {\n        'experiment': experiment_df,\n        'trials': trials_df,\n        'responses': responses_df,\n        'events': events_df\n    }\n\n\nparsed_data = log_eg.parse_trials_and_events_cog_bias()\ndataframes = convert_parsed_data_to_dataframes(parsed_data)\n\n# Now you can access each DataFrame\nexperiment_df = dataframes['experiment']\ntrials_df = dataframes['trials']\nresponses_df = dataframes['responses']\nevents_df = dataframes['events']\n\n\nexperiment_df\n\n\ntrials_df\n\n\nevents_df\n\n\nresponses_df\n\n\n\n\nBringing this all together and putting the data in DuckDB database\n\nlen(extra_excludes)\n\n\ndef parse_and_load_logfiles_to_database(logfiles_to_process: List[str], con, subject_df: pd.DataFrame):\n    experiment_id = 0\n    trial_id = 0\n    response_id = 0\n    event_id = 0\n    skipped_logfiles = []  # New list to store skipped log files\n\n    for log_filename in sorted(logfiles_to_process):\n        logger.info(f\"Processing {experiment_id}: {logs.path}/{log_filename}\")\n        logfile = Logfile(f\"{logs.path}/{log_filename}\")\n        \n        # Parse filename components\n        try:\n            exp_details = logfile.parse_filename_components_cog_bias()\n        except ValueError as e:\n            logger.warning(f\"Skipping {log_filename}: {str(e)}\")\n            skipped_logfiles.append(log_filename)\n            continue\n\n        exp_details.ExperimentID = experiment_id\n\n        # Look up additional details from subject_df\n        if exp_details.SubjectName:\n            subject_row = subject_df[subject_df[\"subject_name\"] == exp_details.SubjectName]\n            if not subject_row.empty:\n                exp_details.Cohort = subject_row[\"Cohort\"].iloc[0]\n                exp_details.SubjectNumber = int(subject_row[\"subject_number\"].iloc[0])\n            else:\n                logger.warning(f\"Subject {exp_details.SubjectName} not found in subject_df. Skipping this file.\")\n                skipped_logfiles.append(log_filename)\n                continue\n        else:\n            logger.warning(f\"Unable to determine SubjectName for {log_filename}. Skipping this file.\")\n            skipped_logfiles.append(log_filename)\n            continue\n\n        # Parse parameters\n        exp_details.Parameters = logfile.parse_parameters()\n\n        # Parse trials and events\n        parsed_data = logfile.parse_trials_and_events_cog_bias()\n        if parsed_data is None:\n            logger.warning(f\"Skipping {log_filename} due to parsing error\")\n            skipped_logfiles.append(log_filename)\n            continue\n\n        trials = parsed_data['trials']\n        responses = parsed_data['responses']\n        events = parsed_data['events']\n\n        # Insert experiment record\n        logfile.insert_record_to_database(con, exp_details)\n\n        # Process and insert trials first\n        trial_id_mapping = {}  # To map original TrialIDs to new ones\n        for trial in trials:\n            original_trial_id = trial.TrialID\n            trial.TrialID = trial_id\n            trial.ExperimentID = experiment_id\n            logfile.insert_record_to_database(con, trial)\n            trial_id_mapping[original_trial_id] = trial_id\n            trial_id += 1\n\n        # Process and insert responses\n        for response in responses:\n            response.ResponseID = response_id\n            response.TrialID = trial_id_mapping[response.TrialID]\n            logfile.insert_record_to_database(con, response)\n            response_id += 1\n\n        # Process and insert events\n        for event in events:\n            event.EventID = event_id\n            if event.TrialID is not None:\n                event.TrialID = trial_id_mapping[event.TrialID]\n            logfile.insert_record_to_database(con, event)\n            event_id += 1\n\n        experiment_id += 1\n\n        # print(f\"Trials: {trial_id}, responses: {response_id}, events {event_id}\")\n\n    return len(logfiles_to_process), trial_id, response_id, event_id, skipped_logfiles\n\n\nnum_files, num_trials, num_responses, num_events, skipped_files = parse_and_load_logfiles_to_database(logfiles_to_process, con, subject_df)\n\n\nlogger.info(f\"# Loaded - Experiments: {num_files-len(skipped_files)}, Trials: {num_trials}, Events: {num_events}, Responses: {num_responses}\")\n\nprint(f\"Number of files processed: {num_files}\")\nprint(f\"Number of trials: {num_trials}\")\nprint(f\"Number of responses: {num_responses}\")\nprint(f\"Number of events: {num_events}\")\nprint(f\"Number of skipped files: {len(skipped_files)}\")\n\n\n# Cross-check queries\n\ncon.sql(\"SHOW TABLES;\")\n\n\ncon.sql(\"SELECT COUNT(*) FROM ExperimentCBs\")\n\n\ncon.sql(\"SELECT COUNT(*) FROM TrialCBs\")\n\n\ncon.sql(\"SELECT COUNT(*) FROM ResponseCBs\")\n\n\ncon.sql(\"SELECT COUNT(*) FROM EventCBs\")\n\n\nexperiments_df = con.sql(\"SELECT * FROM ExperimentCBs\").df()\n\n\ncon.sql(\"SELECT DISTINCT ExperimentType FROM ExperimentCBs ORDER BY ExperimentType\")\n\n\ncon.sql(\"SELECT DISTINCT SubjectName FROM ExperimentCBs ORDER BY SubjectName\")\n\n\ncon.sql(\"SELECT * FROM TrialCBs\")\n\n\ncon.sql(\"SELECT DISTINCT TrialStartTime FROM TrialCBs\")\n\n\nevents_ddb = con.sql(\"SELECT * FROM EventCBs\").df()\n\n\nevent_type_ddb = con.sql(\"SELECT DISTINCT EventType FROM EventCBs ORDER BY EventType\").df()\n\n\ncon.close()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RPE and CB data analysis notebooks",
    "section": "",
    "text": "These are the notebooks that perform (for both Reward Prediction Error (RPE) and Cognitive Bias (CB) experiments):\n\nA reconciliation of the the log files whose data will be loaded into the databases.\nThe parsing of the log files into the respective databases (one for RPE and one for CB data)\nSample queries for using the data in both DuckDB databases.\nAllowing for manual corrections to the CB data.\n\n\n\n\n\n\n\n\n\n\nLogfile Reconciliation - Reward Prediction Error (RPE)\n\n\nExperimental data analysis (Oct / Nov 2023)\n\n\nReconciliation of the logfiles generated during the experiments to determine which log files to load to the database. \n\n\n\n\n\nOct 10, 2024\n\n\nCathrynne Henshall, Michael J. Booth\n\n\n10/10/24, 4:28:32 PM\n\n\n\n\n\n\n\nLogfiles to RPE database\n\n\nExperimental data analysis (Oct / Nov 2023)\n\n\nParsing of the RPE logfiles into DuckDB database for analysis. \n\n\n\n\n\nOct 10, 2024\n\n\nCathrynne Henshall, Michael J. Booth\n\n\n10/10/24, 4:28:32 PM\n\n\n\n\n\n\n\nLogfile Reconciliation - Cognitive Bias (CB)\n\n\nExperimental data analysis (Oct / Nov 2023)\n\n\nReconciliation of the CB logfiles generated during the experiments. \n\n\n\n\n\nOct 10, 2024\n\n\nCathrynne Henshall, Michael J. Booth\n\n\n10/10/24, 4:28:32 PM\n\n\n\n\n\n\n\nLogfiles to CB database\n\n\nExperimental data analysis (Oct / Nov 2023)\n\n\nParsing of the logfiles into DuckDB database for analysis. \n\n\n\n\n\nOct 10, 2024\n\n\nCathrynne Henshall, Michael J. Booth\n\n\n10/10/24, 4:28:32 PM\n\n\n\n\n\n\n\nRPE Database Example Queries\n\n\nExperimental data analysis (Oct / Nov 2023)\n\n\nQuerying the data from the database \n\n\n\n\n\nOct 10, 2024\n\n\nCathrynne Henshall, Michael J. Booth\n\n\n10/10/24, 4:28:32 PM\n\n\n\n\n\n\n\nCB Database Example Queries\n\n\nExperimental data analysis (Oct / Nov 2023)\n\n\nQuerying the data from the database \n\n\n\n\n\nOct 10, 2024\n\n\nCathrynne Henshall, Michael J. Booth\n\n\n10/10/24, 4:28:32 PM\n\n\n\n\n\n\n\nCorrections to database - Cognitive Bias (CB)\n\n\nExperimental data analysis (Oct / Nov 2023)\n\n\nParsing of the logfiles into DuckDB database for analysis. \n\n\n\n\n\nOct 10, 2024\n\n\nCathrynne Henshall, Michael J. Booth\n\n\n10/10/24, 4:28:32 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "logfile-to-database-RPE.html",
    "href": "logfile-to-database-RPE.html",
    "title": "Logfiles to RPE database",
    "section": "",
    "text": "This notebook loads and parses log file data for the RPE experiments into a database.\nThe primary objectives are:\n\nTo load the log files determined to be included in the analysis in notebooks/logfile-reconciliation-RPE.ipynb.\nTo parse and extract the data for each experiment from each corresponding log file and push this data into a DuckDB database.\n\nThe notebook can be “rendered” using the very powerful Quarto publication tool to produce HTML (webpage) output from the notebook. The tags # | echo: false suppress the output of the Python code in a cell and only display the results (output from each cell.) See https://quarto.org/docs/reference/formats/html.html#execution for details.\n\n\n\n\n\n\n\n\nAs determined in logfiles-reconciliation.ipynb\n\n\n\nAnalysis code is in logfiles.py. Explore parsing log information for specific example defined below.\n\n\n\nThese are created as dataclasses in Python (in logfiles.py) and the corresponding TABLES in SQL (sql/create_experiment_tables_ddb.sql).\n\n\n\n\n\n\n\n\nParameters are stored a a Dict in the Experiment class (see above). These get mapped to a JSON field in the database.\n\n\n\n\n\n\n\n\n\n\n\nExample of Events for each trial number\n\n\n\n\n\nTODO (CH): Should record why these extra log files were excluded and ideally move back into the reconciliation notebook. From memory these could not be parsed for some reason.\n\nlogfiles_to_process\n\n\n\nThe companion notebook database-queries.ipynb explores the database in more detail.\n\n# Cross-check queries\n\ncon.sql(\"SHOW TABLES;\")\n\n\n\n\n\ncon.sql(\"SELECT COUNT(*) as n_experiment FROM Experiments\")\n\n\ncon.sql(\"SELECT DISTINCT ExperimentType FROM Experiments\")\n\n\ncon.sql(\"SELECT DISTINCT SubjectName FROM Experiments ORDER BY SubjectName\")\n\n\ncon.sql(\"SELECT DISTINCT Cohort FROM Experiments\")\n\n\n# Cross-check against inputs - OK\n\ncon.sql(\n    \"SELECT DISTINCT Cohort, SubjectNumber, SubjectName FROM Experiments ORDER BY SubjectNumber\"\n)  # .df().to_csv(HORSE_ORDER_PATH / \"cross_check_subjects.csv\")\n\n\n\n\n\ncon.sql(\"SELECT COUNT(*) as n_trial FROM Trials\")\n\n\n\n\n\ncon.sql(\"SELECT COUNT(*) as n_event FROM Events\")\n\n\ncon.sql(\"SELECT COUNT (DISTINCT EventType) as n_event_type FROM Events\")\n\n\n\n\n\ncon.sql(\n    \"\"\"\n    SELECT COUNT (DISTINCT Parameters) \n    FROM\n        Experiments\n\"\"\"\n)\n\n\n# TODO - Count elements in the JSON ()\ncon.sql(\n    \"\"\"\n    SELECT DISTINCT json_keys(Parameters) as num_items, ExperimentType FROM Experiments\n        \"\"\"\n)\n\n\n# SQL to retrieve specific parameter (INITIAL_DELAY) from specific ExperimentID\n\ncon.sql(\n    \"\"\"\n    SELECT \n        ExperimentID,\n        JSON_EXTRACT(Parameters, '$.INITIAL_DELAY') AS InitialDelay\n    FROM \n        Experiments\n    WHERE\n        ExperimentID = 42\n\"\"\"\n)\n\n\ncon.sql(\n    \"\"\"\n    SELECT DISTINCT Parameters \n    FROM\n        Experiments\n        \"\"\"\n).df()\n\n\nparameter_df = con.sql(\"SELECT Parameters FROM Experiments\").df()\n\n\nparameter_df[\"Parameters\"].nunique()\n\n\nparameter_df[\"Parameters\"].unique()\n\n\ncon.sql(\n    \"\"\"\nSELECT \n    *\nFROM \n    Experiments\nWHERE\n    JSON_EXTRACT(Parameters, '$.MODE') = '\"test\"'\n\"\"\"\n)\n\n\ndef get_parameter_for_experimentid(parameter, id):\n    return con.sql(\n        f\"\"\"\n    SELECT \n        JSON_EXTRACT(Parameters, '$.{parameter}') AS InitialDelay\n    FROM \n        Experiments\n    WHERE\n        ExperimentID = {id}\n\"\"\"\n    )\n\n\nget_parameter_for_experimentid(\"SERVO_MODE\", 42)\n\n\nget_parameter_for_experimentid(\"MODE\", 0)"
  },
  {
    "objectID": "logfile-to-database-RPE.html#purpose",
    "href": "logfile-to-database-RPE.html#purpose",
    "title": "Logfiles to RPE database",
    "section": "",
    "text": "This notebook loads and parses log file data for the RPE experiments into a database.\nThe primary objectives are:\n\nTo load the log files determined to be included in the analysis in notebooks/logfile-reconciliation-RPE.ipynb.\nTo parse and extract the data for each experiment from each corresponding log file and push this data into a DuckDB database.\n\nThe notebook can be “rendered” using the very powerful Quarto publication tool to produce HTML (webpage) output from the notebook. The tags # | echo: false suppress the output of the Python code in a cell and only display the results (output from each cell.) See https://quarto.org/docs/reference/formats/html.html#execution for details.\n\n\n\n\n\n\n\n\nAs determined in logfiles-reconciliation.ipynb\n\n\n\nAnalysis code is in logfiles.py. Explore parsing log information for specific example defined below.\n\n\n\nThese are created as dataclasses in Python (in logfiles.py) and the corresponding TABLES in SQL (sql/create_experiment_tables_ddb.sql).\n\n\n\n\n\n\n\n\nParameters are stored a a Dict in the Experiment class (see above). These get mapped to a JSON field in the database.\n\n\n\n\n\n\n\n\n\n\n\nExample of Events for each trial number\n\n\n\n\n\nTODO (CH): Should record why these extra log files were excluded and ideally move back into the reconciliation notebook. From memory these could not be parsed for some reason.\n\nlogfiles_to_process\n\n\n\nThe companion notebook database-queries.ipynb explores the database in more detail.\n\n# Cross-check queries\n\ncon.sql(\"SHOW TABLES;\")\n\n\n\n\n\ncon.sql(\"SELECT COUNT(*) as n_experiment FROM Experiments\")\n\n\ncon.sql(\"SELECT DISTINCT ExperimentType FROM Experiments\")\n\n\ncon.sql(\"SELECT DISTINCT SubjectName FROM Experiments ORDER BY SubjectName\")\n\n\ncon.sql(\"SELECT DISTINCT Cohort FROM Experiments\")\n\n\n# Cross-check against inputs - OK\n\ncon.sql(\n    \"SELECT DISTINCT Cohort, SubjectNumber, SubjectName FROM Experiments ORDER BY SubjectNumber\"\n)  # .df().to_csv(HORSE_ORDER_PATH / \"cross_check_subjects.csv\")\n\n\n\n\n\ncon.sql(\"SELECT COUNT(*) as n_trial FROM Trials\")\n\n\n\n\n\ncon.sql(\"SELECT COUNT(*) as n_event FROM Events\")\n\n\ncon.sql(\"SELECT COUNT (DISTINCT EventType) as n_event_type FROM Events\")\n\n\n\n\n\ncon.sql(\n    \"\"\"\n    SELECT COUNT (DISTINCT Parameters) \n    FROM\n        Experiments\n\"\"\"\n)\n\n\n# TODO - Count elements in the JSON ()\ncon.sql(\n    \"\"\"\n    SELECT DISTINCT json_keys(Parameters) as num_items, ExperimentType FROM Experiments\n        \"\"\"\n)\n\n\n# SQL to retrieve specific parameter (INITIAL_DELAY) from specific ExperimentID\n\ncon.sql(\n    \"\"\"\n    SELECT \n        ExperimentID,\n        JSON_EXTRACT(Parameters, '$.INITIAL_DELAY') AS InitialDelay\n    FROM \n        Experiments\n    WHERE\n        ExperimentID = 42\n\"\"\"\n)\n\n\ncon.sql(\n    \"\"\"\n    SELECT DISTINCT Parameters \n    FROM\n        Experiments\n        \"\"\"\n).df()\n\n\nparameter_df = con.sql(\"SELECT Parameters FROM Experiments\").df()\n\n\nparameter_df[\"Parameters\"].nunique()\n\n\nparameter_df[\"Parameters\"].unique()\n\n\ncon.sql(\n    \"\"\"\nSELECT \n    *\nFROM \n    Experiments\nWHERE\n    JSON_EXTRACT(Parameters, '$.MODE') = '\"test\"'\n\"\"\"\n)\n\n\ndef get_parameter_for_experimentid(parameter, id):\n    return con.sql(\n        f\"\"\"\n    SELECT \n        JSON_EXTRACT(Parameters, '$.{parameter}') AS InitialDelay\n    FROM \n        Experiments\n    WHERE\n        ExperimentID = {id}\n\"\"\"\n    )\n\n\nget_parameter_for_experimentid(\"SERVO_MODE\", 42)\n\n\nget_parameter_for_experimentid(\"MODE\", 0)"
  },
  {
    "objectID": "logfile-to-database-RPE.html#export-database-tables-experiments-trials-and-events-to-single-.xlsx-and-.csv-3-files",
    "href": "logfile-to-database-RPE.html#export-database-tables-experiments-trials-and-events-to-single-.xlsx-and-.csv-3-files",
    "title": "Logfiles to RPE database",
    "section": "Export database tables (Experiments, Trials and Events) to single .xlsx and .csv (3 files)",
    "text": "Export database tables (Experiments, Trials and Events) to single .xlsx and .csv (3 files)\nTo enable detailed review of the data in human-readable formats."
  },
  {
    "objectID": "upload-to-motherduck.html",
    "href": "upload-to-motherduck.html",
    "title": "Upload to MotherDuck notebook (OPTIONAL)",
    "section": "",
    "text": "This Jupyter Notebook is designed to help you upload a local DuckDB database (file) to the MotherDuck server.\nThis allows you to share and collaborate on your database with others easily.\n\nPurpose\nThe main purpose of this notebook is to:\n\nSet up logging: Customise logging to track the process and any issues.\nLoad secrets: Load necessary secrets (like tokens/password) from a file to authenticate with MotherDuck.\nCheck local database: Verify if a local DuckDB database exists.\n\nUpload database: Copy the local DuckDB database to the MotherDuck server. Interact with the database: Perform SQL queries on the uploaded database.\n\n\nSteps\n\nImport necessary libraries: The notebook imports various libraries required for file handling, database operations, and logging.\nCustomise Logger: Sets up a logger to track the process and any issues.\nLoad Secrets: Loads the MotherDuck token from a .secrets.toml file.\nCheck Local Database: Checks if the local DuckDB database exists.\n\nUpload Database: Copies the local DuckDB database to the MotherDuck server. Interact with the Database: Connects to the uploaded database and performs SQL queries.\n\n\nHow to Use\nEnsure Dependencies: Make sure you have all the required libraries installed. You can install them using pip: Bash loguru Prepare Secrets File: Create a .secrets.toml file in the parent directory of your current working directory. This file should contain your MotherDuck token: ”\nRun the Notebook: Open the notebook in Jupyter and run each cell sequentially. The notebook will: - Set up logging. - Load the MotherDuck token. - Check for the local DuckDB database. - Upload the database to MotherDuck. - Allow you to interact with the uploaded database.\nCollaborate: Once the database is uploaded to MotherDuck, you can share access with your collaborators. They can connect to the database using sharing.\n\nExample Output\n\nDatabase Upload: The notebook will log the creation of the database on MotherDuck.\nSQL Queries: You can run SQL queries on the uploaded database and see the results directly in the notebook.\n\n\n\nTroubleshooting\n\nLogging: Check the log file (DuckDBManager.log) for any errors or issues during the process.\nSecrets: Ensure your .secrets.toml file is correctly formatted and contains the right token.\n\n\nimport os\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport duckdb\nimport toml\nfrom loguru import logger\n\n\ndef stdout_filter(record):\n    return record[\"level\"].name == \"INFO\" and \"ECHO\" in record[\"message\"]\n\ndef customise_logger(logger_filename=\"DuckDBManager.log\", logger_size=10, filter_func=stdout_filter, level=\"INFO\"):\n    try:\n        logger.remove(0) # Remove default output to sys.stderr\n        logger.add(sys.stdout, filter=filter_func, level=level)     # Add sys.stdout logging of select messages\n        logger.add(logger_filename, rotation=f\"{logger_size} MB\", level=level)  # logs will be written to file.log\n        logger.info(f\"ECHO: Logging to {logger_filename}: size = {logger_size} MB\")\n        logger.info(\"ECHO: Limited logging to stdout using `stdout_filter`\")\n    except Exception as e:\n        logger.error(f\"Error with customising logger: {e}\")\n\n\ncustomise_logger()\n\n\nlogger.info(f\"ECHO - Local DuckDB version: {duckdb.__version__}\")\n\n\nSECRETS_TOML = Path.cwd().parent / \".secrets.toml\"\n\n\ndef set_md_token(secrets_file):\n    \"\"\"\n    Load secrets from a file and set the motherduck_token environment variable.\n    \"\"\"\n    if \"motherduck_token\" not in os.environ:\n        try:\n            secrets = toml.load(secrets_file)\n            os.environ[\"motherduck_token\"] = secrets[\"MD_TOKEN\"]\n            logger.info(f\"ECHO: Secrets loaded from {secrets_file} and MotherDuck environment variable set\")\n        except Exception as e:\n            logger.error(f\"Failed to load secrets: {e}\")\n            raise\n    else:\n        logger.info(\"ECHO: MD environment variable already set.\")\n\n\ndef check_local_duckdb_database():\n    \"\"\"\n    Checks if a local DuckDB database exists. It returns the path to the database if it exists, and None otherwise.\n    \"\"\"\n    DATA_DIR = Path(\"../data\")\n    EXPERIMENT_TYPE = \"RPE\"  # or CB\n    assert DATA_DIR.exists()\n    local_ddb = DATA_DIR / f\"Experiments_{EXPERIMENT_TYPE}_2023_Q4.ddb\"  # DuckDB database name\n\n    logger.info(f\"ECHO - Database file: {local_ddb.resolve()}\")\n\n    if local_ddb.exists():\n        return local_ddb\n    else:\n        return None\n\n\nlocal_ddb = check_local_duckdb_database()\n\n\ndef copy_local_ddb_to_md(local_ddb, add_date_suffix=True):\n    set_md_token(SECRETS_TOML)\n    try:\n        if add_date_suffix:\n            date_suffix = datetime.now().strftime(\"_%d_%B_%Y\")\n        else:\n            date_suffix = \"\"\n        remote_db_name = f\"{local_ddb.stem.lower()}{date_suffix}\"\n        con_md = duckdb.connect(\"md:\")\n        con_md.sql(\"USE default_db;\")    # To avoid trying to drop database if it is the default in MD\n        con_md.sql(f\"DROP DATABASE IF EXISTS {remote_db_name};\")\n        con_md.sql(f\"CREATE OR REPLACE DATABASE {remote_db_name} FROM '{str(local_ddb)}';\")\n        logger.info(f\"Created database {remote_db_name} on https://motherduck.com\")\n        return remote_db_name, con_md\n    except Exception as e:\n        logger.error(f\"Failed to copy local database to MotherDuck server: {e}\")\n        raise\n\n\nremote_db_name, con_md = copy_local_ddb_to_md(local_ddb, add_date_suffix=True)\n\n\ncon_md.sql(\"PRAGMA database_list;\")\n\n\nremote_db_name\n\n\ncon = duckdb.connect(\"md:\")\n\n\ncon.sql(f\"USE '{remote_db_name}'\")\n\n\ncon.sql(\"SELECT * FROM Experiments\")"
  },
  {
    "objectID": "database-queries-RPE.html",
    "href": "database-queries-RPE.html",
    "title": "RPE Database Example Queries",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log file data from the horse behavioural experiments conducted in October and November 2023 that are loaded into a local DuckDB database using logfile-to-database-RPE.ipynb.\nIt facilitates loading and querying the data from the database using some example SQL queries.\n\n\n\n\n\n\n\n\n\n\n\nShow the tables in the database - should be Events, Experiments and Trials.\n\n# Cross-check queries\n\ncon.sql(\"SHOW TABLES;\")\n\n\n\n\n\ncon.sql(\"SELECT DISTINCT EventType FROM Events ORDER BY EventType\")\n\n\n\n\n\nexperiments_df = con.sql(\"SELECT * FROM Experiments\").df()\n\n\nexperiments_df\n\n\n\n\n\ncon.sql(\n    \"\"\"\n    SELECT\n        ExperimentID, \n        Cohort,\n        SubjectName, \n        SubjectNumber, \n        SessionNumber,\n        ExperimentType, \n        Comment, \n        DateTime, \n        LogFileName\n    FROM Experiments \n    WHERE ExperimentID = 128;\n\"\"\"\n).df()\n\n\n\n\n\ncon.sql(\"\"\"\n    SELECT \n        -- Trials.TrialID,\n        Trials.TrialNumber,\n        -- TrialStartTime,\n        -- TrialEndTime,\n        -- EventID, \n        EventNumber, \n        EventTime, \n        EventType,\n        EventElapsedTime\n    FROM Experiments\n    INNER JOIN Trials ON Experiments.ExperimentID = Trials.ExperimentID\n    INNER JOIN Events ON Trials.TrialID = Events.TrialID\n    WHERE Experiments.ExperimentID = 128;\n\"\"\").df()\n\n\n\n\n\ncon.sql(\"\"\"\n    SELECT \n        -- Experiments.SubjectName,\n        -- Experiments.SessionNumber,\n        -- Trials.TrialID,\n        Trials.TrialNumber,\n        -- TrialStartTime,\n        -- TrialEndTime,\n        -- EventID, \n        EventNumber, \n        EventTime, \n        EventType,\n        EventElapsedTime\n    FROM Experiments\n    INNER JOIN Trials ON Experiments.ExperimentID = Trials.ExperimentID\n    INNER JOIN Events ON Trials.TrialID = Events.TrialID\n    WHERE Experiments.SubjectName = 'gio' AND  Experiments.SessionNumber = 2;\n\"\"\").df()\n\n\n\n\nTo avoid file lock errors.\n\ncon.close()"
  },
  {
    "objectID": "database-queries-RPE.html#purpose-of-this-notebook",
    "href": "database-queries-RPE.html#purpose-of-this-notebook",
    "title": "RPE Database Example Queries",
    "section": "",
    "text": "This notebook serves as an exploratory tool for examining the log file data from the horse behavioural experiments conducted in October and November 2023 that are loaded into a local DuckDB database using logfile-to-database-RPE.ipynb.\nIt facilitates loading and querying the data from the database using some example SQL queries.\n\n\n\n\n\n\n\n\n\n\n\nShow the tables in the database - should be Events, Experiments and Trials.\n\n# Cross-check queries\n\ncon.sql(\"SHOW TABLES;\")\n\n\n\n\n\ncon.sql(\"SELECT DISTINCT EventType FROM Events ORDER BY EventType\")\n\n\n\n\n\nexperiments_df = con.sql(\"SELECT * FROM Experiments\").df()\n\n\nexperiments_df\n\n\n\n\n\ncon.sql(\n    \"\"\"\n    SELECT\n        ExperimentID, \n        Cohort,\n        SubjectName, \n        SubjectNumber, \n        SessionNumber,\n        ExperimentType, \n        Comment, \n        DateTime, \n        LogFileName\n    FROM Experiments \n    WHERE ExperimentID = 128;\n\"\"\"\n).df()\n\n\n\n\n\ncon.sql(\"\"\"\n    SELECT \n        -- Trials.TrialID,\n        Trials.TrialNumber,\n        -- TrialStartTime,\n        -- TrialEndTime,\n        -- EventID, \n        EventNumber, \n        EventTime, \n        EventType,\n        EventElapsedTime\n    FROM Experiments\n    INNER JOIN Trials ON Experiments.ExperimentID = Trials.ExperimentID\n    INNER JOIN Events ON Trials.TrialID = Events.TrialID\n    WHERE Experiments.ExperimentID = 128;\n\"\"\").df()\n\n\n\n\n\ncon.sql(\"\"\"\n    SELECT \n        -- Experiments.SubjectName,\n        -- Experiments.SessionNumber,\n        -- Trials.TrialID,\n        Trials.TrialNumber,\n        -- TrialStartTime,\n        -- TrialEndTime,\n        -- EventID, \n        EventNumber, \n        EventTime, \n        EventType,\n        EventElapsedTime\n    FROM Experiments\n    INNER JOIN Trials ON Experiments.ExperimentID = Trials.ExperimentID\n    INNER JOIN Events ON Trials.TrialID = Events.TrialID\n    WHERE Experiments.SubjectName = 'gio' AND  Experiments.SessionNumber = 2;\n\"\"\").df()\n\n\n\n\nTo avoid file lock errors.\n\ncon.close()"
  }
]