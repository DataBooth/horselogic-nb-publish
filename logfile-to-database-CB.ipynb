{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Logfiles to CB database\"\n",
    "subtitle: \"Experimental data analysis (Oct / Nov 2023)\"\n",
    "date: \"now\"\n",
    "date-format: \"ddd MMM D, YYYY h:mm A\"\n",
    "author:\n",
    "  - name: \"Cathrynne Henshall\"\n",
    "    email: ponies@hillydale.com.au\n",
    "    affiliation: \n",
    "      - name: Charles Sturt University\n",
    "        url: www.csu.edu.au\n",
    "  - name: \"Michael J. Booth\"\n",
    "    email: michael@databooth.com.au\n",
    "    affiliation: \n",
    "      - name: DataBooth\n",
    "        url: www.databooth.com.au\n",
    "abstract: >\n",
    "  Parsing of the logfiles into DuckDB database for analysis.\n",
    "title-block-banner: true\n",
    "format:\n",
    "  html:\n",
    "    code-tools: true\n",
    "    code-fold: false\n",
    "    toc: true\n",
    "#  docx:\n",
    "#    toc: true # Include a table of contents\n",
    "execute:\n",
    "  enabled: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of This Notebook\n",
    "\n",
    "This notebook serves as an exploratory tool for examining the log files produced during the horse behavioural experiments conducted in October and November 2023. It facilitates the experimentation with text parsing techniques on the files before they are imported into a database. The primary objectives are:\n",
    "\n",
    "\n",
    "> To conduct experiments with regular expressions ([`regex`](https://docs.python.org/3/howto/regex.html)) aimed at extracting pertinent data and fields from the log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from loguru import logger\n",
    "\n",
    "from horse_logic.logfiles import EventCB, ExperimentCB, Logfile, Logs, TrialCB\n",
    "from horse_logic.utils import (\n",
    "    create_tables_from_sql_file,\n",
    "    display_class_definition,\n",
    "    export_data_to_csv,\n",
    "    set_custom_logger_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "import itables.options as opt\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(all_interactive=True, connected=False)  # Display Pandas dataframes in a more friendly paginated manner\n",
    "opt.pageLength = 20  # Display 20 rows per page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "set_custom_logger_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# Custom notebook analysis functions\n",
    "\n",
    "def display_sessions_by_subject(df_out, subject_df):\n",
    "    session_summary = []\n",
    "    for n_subject, subject_name in enumerate(subject_df[\"subject_name\"]):\n",
    "        df_ = df_out.drop(columns=[\"suffix\", \"subject_name\"])[df_out[\"subject_name\"] == subject_name]\n",
    "        n_session = len(df_)\n",
    "        session_summary.append((n_subject+1, subject_name.capitalize(), n_session))\n",
    "        if n_session == 0:\n",
    "            logger.info(f\"Subject {subject_name}: No experiments conducted\")\n",
    "        else:\n",
    "            df_.loc[:, \"time_dff\"] = df_[\"datetime\"].diff()\n",
    "            display(Markdown(f\"### {n_subject+1}. {subject_name.capitalize()}: {n_session} session(s)\"))\n",
    "            display(df_)\n",
    "    return pd.DataFrame(session_summary, columns=[\"Subject number\", \"Subject name\", \"Session count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Subject information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "def get_subject_info():\n",
    "    HORSE_ORDER_XLSX = \"Cohort data for MB.xlsx\"\n",
    "    HORSE_ORDER_PATH = Path(\"../docs/from_CH\") \n",
    "    HORSE_ORDER_FILEPATH = HORSE_ORDER_PATH / HORSE_ORDER_XLSX\n",
    "\n",
    "    if HORSE_ORDER_FILEPATH.exists():\n",
    "        subject_df = pd.read_excel(HORSE_ORDER_FILEPATH)\n",
    "        logger.info(f\"Loaded horse order info from: {HORSE_ORDER_FILEPATH}\")\n",
    "\n",
    "        subject_df.rename({\"No\": \"subject_number\", \"Horse\": \"subject_name\"}, axis=1, inplace=True)  # Rename columns\n",
    "        subject_df[\"subject_name\"] = subject_df[\"subject_name\"].str.lower()     # Ensure lower case names for later subject lookup\n",
    "        return subject_df\n",
    "    else:\n",
    "        logger.error(f\"Horse order info not found: {HORSE_ORDER_FILEPATH}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = get_subject_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log file reconciliation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory information: log file data and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "LOGFILES_DIR = \"../data/results/zips/cb_data\"\n",
    "\n",
    "logger.info(f\"Logfiles dir: {LOGFILES_DIR}\")\n",
    "assert Path(LOGFILES_DIR).exists()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "EXPERIMENT_TYPE = \"CB\"    # or CB\n",
    "\n",
    "assert DATA_DIR.exists()\n",
    "\n",
    "DATA_DB  = DATA_DIR / f\"Experiments_{EXPERIMENT_TYPE}_2023-Q4.ddb\"  # DuckDB database name\n",
    "db_exists = DATA_DB.exists()\n",
    "\n",
    "logger.info(f\"Database file: {DATA_DB.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "OUTPUT_DIR = DATA_DIR / f\"results/{EXPERIMENT_TYPE}\"\n",
    "\n",
    "assert OUTPUT_DIR.exists()\n",
    "logger.info(f\"Outputs dir: {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load logfiles from CSV list of included files\n",
    "\n",
    "As determined in `logfiles-reconciliation-cb.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "logs = Logs(path=LOGFILES_DIR)\n",
    "logs.load_specific_logfiles_from_csv(Path(OUTPUT_DIR) / \"all_included_files_CB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "logger.info(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `regex` parsing of a single example log file\n",
    "\n",
    "Analysis code is in `logfiles.py`. Explore parsing log information for specific example defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUst choose one of the file names for testing\n",
    "\n",
    "log_eg = [log.file_name for log in logs.logfiles][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override with specific log file\n",
    "\n",
    "log_eg = \"Experiment_2023-11-15T00:39:50.244185_clover_nan_Test Type 1.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "logfile_eg = f\"{logs.path}/{log_eg}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# Load example log file into Logfile class for processing\n",
    "log_eg = Logfile(logfile_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "print(f\"Example log file for regex parsing: {log_eg.file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures\n",
    "\n",
    "These are created as `dataclasses` in Python (in `logfiles.py`) and TABLES in SQL (`sql/create_experiment_tables_ddb.sql`).\n",
    "\n",
    "#### Experiment table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "display_class_definition(ExperimentCB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.g. Parse experiment details and comment from log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "pprint(log_eg.parse_filename_components_cog_bias())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trials and Events tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_eg.parse_filename_components_cog_bias().LogFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "display_class_definition(TrialCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "display_class_definition(EventCB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e.g. Parsing of trials and events info from log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_eg.parse_trials_and_events_cog_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_eg.parse_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "def convert_parsed_data_to_dataframes(parsed_data):\n",
    "    if parsed_data is None:\n",
    "        return None\n",
    "\n",
    "    # Convert experiment to DataFrame\n",
    "    experiment_df = pd.DataFrame([asdict(parsed_data['experiment'])])\n",
    "\n",
    "    # Convert trials to DataFrame\n",
    "    trials_df = pd.DataFrame([asdict(trial) for trial in parsed_data['trials']])\n",
    "\n",
    "    # Convert responses to DataFrame\n",
    "    responses_df = pd.DataFrame([asdict(response) for response in parsed_data['responses']])\n",
    "\n",
    "    # Convert events to DataFrame\n",
    "    events_df = pd.DataFrame([asdict(event) for event in parsed_data['events']])\n",
    "\n",
    "    return {\n",
    "        'experiment': experiment_df,\n",
    "        'trials': trials_df,\n",
    "        'responses': responses_df,\n",
    "        'events': events_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = log_eg.parse_trials_and_events_cog_bias()\n",
    "dataframes = convert_parsed_data_to_dataframes(parsed_data)\n",
    "\n",
    "# Now you can access each DataFrame\n",
    "experiment_df = dataframes['experiment']\n",
    "trials_df = dataframes['trials']\n",
    "responses_df = dataframes['responses']\n",
    "events_df = dataframes['events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# Export an example of the current parsing of the Trial/Event info for one sample experiment\n",
    "\n",
    "export_data_to_csv(trials_df, \"Example of parsing of the Trial/Event info for one sample experiment\", log_eg.parse_filename_components_cog_bias().LogFileName.replace(\" \", \"\").replace(\".log\", \".csv\"), trials_df.columns.to_list())\n",
    "\n",
    "trials_df.to_excel(\"Experiment_2023-11-15T00:39:50.244185_clover_nan_Test Type 1.log.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing this all together and putting the data in DuckDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "if DATA_DB.exists():  # remove database file if it exists\n",
    "    logger.info(f\"Deleted existing database file: {DATA_DB}\")\n",
    "    DATA_DB.unlink()\n",
    "    \n",
    "con = duckdb.connect(database=str(DATA_DB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "create_tables_from_sql_file(con, '../sql/create_cb_experiment_tables_ddb.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "# These files where all excluded by the parsing function (and have been added here explicitly)\n",
    "\n",
    "extra_excludes = [\n",
    "    'Experiment_2023-10-05T00:33:20.982985_bonnie_34_Training - fixed.log',\n",
    "    'Experiment_2023-10-05T00:34:54.463462_bonnie_35_Training - fixed.log',\n",
    "    'Experiment_2023-10-05T00:36:53.748319_bonnie_36_Training randomised Type 1.log',\n",
    "    'Experiment_2023-10-05T00:37:20.038515_bonnie_37_Training randomised Type 2.log',\n",
    "    'Experiment_2023-10-05T00:38:53.793295_bonnie_38_Training - fixed.log',\n",
    "    'Experiment_2023-10-05T00:40:07.655743_bonnie_39_Training - fixed.log',\n",
    "    'Experiment_2023-10-05T00:40:27.842920_bonnie_40_Test Type 1.log',\n",
    "    'Experiment_2023-10-05T00:40:48.563909_bonnie_41_Test Type 2.log',\n",
    "    'Experiment_2023-10-06T08:17:38.338641_bonnie_49_Training randomised Type 1.log',\n",
    "    'Experiment_2023-10-06T11:59:46.930869_bonnie_51_Training - fixed.log',\n",
    "    'Experiment_2023-10-06T13:17:11.425590_ash_1_Training - fixed.log',\n",
    "    'Experiment_2023-10-09T10:02:29.985144_bonnie_57_Training - fixed.log',\n",
    "    'Experiment_2023-10-09T10:03:02.623587_bonnie_58_Training randomised Type 2.log',\n",
    "    'Experiment_2023-10-09T15:08:04.475922_filly_7_Training - fixed.log',\n",
    "    'Experiment_2023-10-09T15:18:10.013061_dougie_4_Training randomised Type 2.log',\n",
    "    'Experiment_2023-10-09T15:24:15.757940_dougie_6_Training randomised Type 2.log',\n",
    "    'Experiment_2023-10-09T17:03:22.507500_bonnie_62_Training - fixed.log',\n",
    "    'Experiment_2023-10-09T18:29:49.279855_molly_5_Test Type 1.log',\n",
    "    'Experiment_2023-10-10T08:33:44.845499_bonnie_65_Test Type 1.log',\n",
    "    'Experiment_2023-10-10T08:38:11.132513_bonnie_66_Test Type 1.log',\n",
    "    'Experiment_2023-10-10T08:39:34.167268_bonnie_67_Test Type 1.log',\n",
    "    'Experiment_2023-10-10T08:42:47.519718_bonnie_68_Test Type 1.log',\n",
    "    'Experiment_2023-10-10T08:43:09.979183_bonnie_69_Test Type 2.log',\n",
    "    'Experiment_2023-10-16T10:38:24.993736_ash_7_Test Type 1.log',\n",
    "    'Experiment_2023-11-12T18:50:43.726156_pumba_nan_Training - fixed.log',\n",
    "    'Experiment_2023-11-13T19:16:57.507097_freya_nan_Training randomised Type 1.log',\n",
    "    'Experiment_2023-11-14T00:11:57.637594_george_4.0_Training randomised Type 2.log',\n",
    "    'Experiment_2023-11-14T11:03:25.088331_nix_6.0_Training - fixed.log',\n",
    "    'Experiment_2023-11-14T12:05:21.440157_dusty_5.0_Training - fixed.log',\n",
    "    'Experiment_2023-11-15T03:51:01.169253_yoshi_7.0_Training - fixed.log'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extra_excludes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "export_data_to_csv(pd.DataFrame(extra_excludes),\n",
    "                   \"Additional log files excluded as bad data\",\n",
    "                   f\"{Path(OUTPUT_DIR) / 'extra_excludes.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "logfiles_to_process = [logfile.file_name for logfile in logs.logfiles if logfile.file_name not in extra_excludes]\n",
    "\n",
    "export_data_to_csv(pd.DataFrame(logfiles_to_process),\n",
    "                   \"List of logfiles to be loaded to database\",\n",
    "                   f\"{Path(OUTPUT_DIR) / 'logfiles-to-database.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_load_logfiles_to_database(logfiles_to_process: List[str], con, subject_df: pd.DataFrame):\n",
    "    experiment_id = 0\n",
    "    trial_id = 0\n",
    "    response_id = 0\n",
    "    event_id = 0\n",
    "    skipped_logfiles = []  # New list to store skipped log files\n",
    "\n",
    "    for log_filename in sorted(logfiles_to_process):\n",
    "        logger.info(f\"Processing {experiment_id}: {logs.path}/{log_filename}\")\n",
    "        logfile = Logfile(f\"{logs.path}/{log_filename}\")\n",
    "        \n",
    "        # Parse filename components\n",
    "        try:\n",
    "            exp_details = logfile.parse_filename_components_cog_bias()\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"Skipping {log_filename}: {str(e)}\")\n",
    "            skipped_logfiles.append(log_filename)\n",
    "            continue\n",
    "\n",
    "        exp_details.ExperimentID = experiment_id\n",
    "\n",
    "        # Look up additional details from subject_df\n",
    "        if exp_details.SubjectName:\n",
    "            subject_row = subject_df[subject_df[\"subject_name\"] == exp_details.SubjectName]\n",
    "            if not subject_row.empty:\n",
    "                exp_details.Cohort = subject_row[\"Cohort\"].iloc[0]\n",
    "                exp_details.SubjectNumber = int(subject_row[\"subject_number\"].iloc[0])\n",
    "            else:\n",
    "                logger.warning(f\"Subject {exp_details.SubjectName} not found in subject_df. Skipping this file.\")\n",
    "                skipped_logfiles.append(log_filename)\n",
    "                continue\n",
    "        else:\n",
    "            logger.warning(f\"Unable to determine SubjectName for {log_filename}. Skipping this file.\")\n",
    "            skipped_logfiles.append(log_filename)\n",
    "            continue\n",
    "\n",
    "        # Parse parameters\n",
    "        exp_details.Parameters = logfile.parse_parameters()\n",
    "\n",
    "        # Parse trials and events\n",
    "        parsed_data = logfile.parse_trials_and_events_cog_bias()\n",
    "        if parsed_data is None:\n",
    "            logger.warning(f\"Skipping {log_filename} due to parsing error\")\n",
    "            skipped_logfiles.append(log_filename)\n",
    "            continue\n",
    "\n",
    "        trials = parsed_data['trials']\n",
    "        responses = parsed_data['responses']\n",
    "        events = parsed_data['events']\n",
    "\n",
    "        # Insert experiment record\n",
    "        logfile.insert_record_to_database(con, exp_details)\n",
    "\n",
    "        # Process and insert trials first\n",
    "        trial_id_mapping = {}  # To map original TrialIDs to new ones\n",
    "        for trial in trials:\n",
    "            original_trial_id = trial.TrialID\n",
    "            trial.TrialID = trial_id\n",
    "            trial.ExperimentID = experiment_id\n",
    "            logfile.insert_record_to_database(con, trial)\n",
    "            trial_id_mapping[original_trial_id] = trial_id\n",
    "            trial_id += 1\n",
    "\n",
    "        # Process and insert responses\n",
    "        for response in responses:\n",
    "            response.ResponseID = response_id\n",
    "            response.TrialID = trial_id_mapping[response.TrialID]\n",
    "            logfile.insert_record_to_database(con, response)\n",
    "            response_id += 1\n",
    "\n",
    "        # Process and insert events\n",
    "        for event in events:\n",
    "            event.EventID = event_id\n",
    "            if event.TrialID is not None:\n",
    "                event.TrialID = trial_id_mapping[event.TrialID]\n",
    "            logfile.insert_record_to_database(con, event)\n",
    "            event_id += 1\n",
    "\n",
    "        experiment_id += 1\n",
    "\n",
    "        # print(f\"Trials: {trial_id}, responses: {response_id}, events {event_id}\")\n",
    "\n",
    "    return len(logfiles_to_process), trial_id, response_id, event_id, skipped_logfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files, num_trials, num_responses, num_events, skipped_files = parse_and_load_logfiles_to_database(logfiles_to_process, con, subject_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"# Loaded - Experiments: {num_files-len(skipped_files)}, Trials: {num_trials}, Events: {num_events}, Responses: {num_responses}\")\n",
    "\n",
    "print(f\"Number of files processed: {num_files}\")\n",
    "print(f\"Number of trials: {num_trials}\")\n",
    "print(f\"Number of responses: {num_responses}\")\n",
    "print(f\"Number of events: {num_events}\")\n",
    "print(f\"Number of skipped files: {len(skipped_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "def export_database_tables(con, output_dir, output_format=\"xlsx\"):\n",
    "    # Get the list of tables in the database\n",
    "    try:\n",
    "        tables = con.sql(\"SHOW TABLES;\").fetchall()\n",
    "\n",
    "        if output_format == \"xlsx\":\n",
    "            # Write to one Excel file by sheet name\n",
    "            with pd.ExcelWriter(f\"{output_dir}/all_tables.xlsx\") as writer:\n",
    "                for table in tables:\n",
    "                    table_name = table[0]\n",
    "                    # Query the table and convert it to a pandas DataFrame\n",
    "                    df = con.table(table_name).to_df()\n",
    "                    df.to_excel(writer, sheet_name=table_name, index=False)\n",
    "            logger.info(f\"Exported database tables to: {output_dir}/all_tables.xlsx\")\n",
    "\n",
    "        elif output_format in [\"csv\", \"parquet\"]:\n",
    "            for table in tables:\n",
    "                table_name = table[0]\n",
    "                # Query the table and convert it to a pandas DataFrame\n",
    "                df = con.table(table_name).to_df()\n",
    "\n",
    "                if output_format == \"csv\":\n",
    "                    # Write to a CSV file\n",
    "                    df.to_csv(f\"{output_dir}/{table_name}.csv\", index=False)\n",
    "                    logger.info(f\"Exported database table to: {output_dir}/{table_name}.csv\")\n",
    "                elif output_format == \"parquet\":\n",
    "                    # Write to a Parquet file\n",
    "                    df.to_parquet(f\"{output_dir}/{table_name}.parquet\", index=False)\n",
    "                    logger.info(f\"Exported database table to: {output_dir}/{table_name}.parquet\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output format: {output_format}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{e} - Database connection not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "export_database_tables(con, OUTPUT_DIR / \"export_tables\", output_format=\"xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-check queries\n",
    "\n",
    "con.sql(\"SHOW TABLES;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT COUNT(*) FROM ExperimentCBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT COUNT(*) FROM TrialCBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT COUNT(*) FROM ResponseCBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT COUNT(*) FROM EventCBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df = con.sql(\"SELECT * FROM ExperimentCBs\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "experiments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT DISTINCT ExperimentType FROM ExperimentCBs ORDER BY ExperimentType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT DISTINCT SubjectName FROM ExperimentCBs ORDER BY SubjectName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT * FROM TrialCBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT DISTINCT TrialStartTime FROM TrialCBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_ddb = con.sql(\"SELECT * FROM EventCBs\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_ddb = con.sql(\"SELECT DISTINCT EventType FROM EventCBs ORDER BY EventType\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "event_type_ddb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
